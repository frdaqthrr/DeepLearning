{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latihan5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpJvYbAL45waG6IKcp5Il5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frdaqthrr/DeepLearning/blob/main/Latihan5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVQG1iadDOKz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AlexNet"
      ],
      "metadata": {
        "id": "221p2MA-zeat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTNTh8-BaPRC",
        "outputId": "3698aea2-31b3-4cb2-fe94-6182833962b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import datasets, layers, models, losses"
      ],
      "metadata": {
        "id": "KnvZQ5IZYM8E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data"
      ],
      "metadata": {
        "id": "L-TqoSEw4qR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()\n",
        "x_train = tf.pad(x_train, [[0, 0], [2,2], [2,2]])/255\n",
        "x_test = tf.pad(x_test, [[0, 0], [2,2], [2,2]])/255\n",
        "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
        "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
        "x_train = tf.repeat(x_train, 3, axis=3)\n",
        "x_test = tf.repeat(x_test, 3, axis=3)\n",
        "x_val = x_train[-2000:,:,:,:]\n",
        "y_val = y_train[-2000:]\n",
        "x_train = x_train[:-2000,:,:,:]\n",
        "y_train = y_train[:-2000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtx1kICJYTbh",
        "outputId": "61aea731-1315-4674-cd59-4315bf50fff2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Model"
      ],
      "metadata": {
        "id": "kheqP6Ux4wnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=x_train.shape[1:]))\n",
        "model.add(layers.Conv2D(96, 11, strides=4, padding='same'))\n",
        "model.add(layers.Lambda(tf.nn.local_response_normalization))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(3, strides=2))\n",
        "model.add(layers.Conv2D(256, 5, strides=4, padding='same'))\n",
        "model.add(layers.Lambda(tf.nn.local_response_normalization))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(3, strides=2))\n",
        "model.add(layers.Conv2D(384, 3, strides=4, padding='same'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Conv2D(384, 3, strides=4, padding='same'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Conv2D(256, 3, strides=4, padding='same'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sncWcohtYTd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f80c67-d408-4b16-aefc-a8c67ed81803"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resizing (Resizing)         (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 56, 56, 96)        34944     \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 56, 56, 96)        0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 56, 56, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 7, 7, 256)         614656    \n",
            "                                                                 \n",
            " lambda_1 (Lambda)           (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 3, 3, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 1, 384)         885120    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1, 1, 384)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 1, 1, 384)         1327488   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 1, 1, 384)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 1, 1, 256)         884992    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 1, 1, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              1052672   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,622,154\n",
            "Trainable params: 21,622,154\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=losses.sparse_categorical_crossentropy,  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sZ0S8ricaNTs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,batch_size=64, epochs=40, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoGctNEDdwzq",
        "outputId": "f9b756b4-00b8-4f83-e737-826990b4a631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "110/907 [==>...........................] - ETA: 29:00 - loss: 2.0680 - accuracy: 0.2087"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
        "axs[0].plot(history.history['loss'])\n",
        "axs[0].plot(history.history['val_loss'])\n",
        "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].legend(['Train', 'Val'])\n",
        "axs[1].plot(history.history['accuracy'])\n",
        "axs[1].plot(history.history['val_accuracy'])\n",
        "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].legend(['Train', 'Val'])"
      ],
      "metadata": {
        "id": "d0rPnhSYYTgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "jSEBQpeCYTkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGG "
      ],
      "metadata": {
        "id": "kmlsoCpF2v2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data"
      ],
      "metadata": {
        "id": "U7j1jSBR3xBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "from tqdm import tqdm\n",
        "from numpy.random import randn\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib.image import imread\n",
        "from keras.preprocessing import image\n",
        "\n",
        "#tf.enable_eager_execution()\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.executing_eagerly()\n",
        "#tf.disable_v2_behavior()\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "data_dir = tf.keras.utils.get_file('flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz', untar=True)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "label_names={'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\n",
        "label_key=['daisy','dandelion','roses','sunflowers','tulips']"
      ],
      "metadata": {
        "id": "wmDwaExsYeAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load images with tf.data"
      ],
      "metadata": {
        "id": "8-R0Hd7037As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = list(data_dir.glob('*/*'))\n",
        "all_images = [str(path) for path in all_images]\n",
        "random.shuffle(all_images)\n",
        "\n",
        "all_labels=[label_names[pathlib.Path(path).parent.name] for path in all_images]\n",
        "\n",
        "data_size=len(all_images)\n",
        "\n",
        "train_test_split=(int)(data_size*0.2)\n",
        "\n",
        "x_train=all_images[train_test_split:]\n",
        "x_test=all_images[:train_test_split]\n",
        "y_train=all_labels[train_test_split:]\n",
        "y_test=all_labels[:train_test_split]\n",
        "\n",
        "IMG_SIZE=160\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def _parse_data(x,y):\n",
        "  image = tf.read_file(x)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image/127.5) - 1\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        " \n",
        "  return image,y\n",
        "\n",
        "def _input_fn(x,y):\n",
        "  ds=tf.data.Dataset.from_tensor_slices((x,y))\n",
        "  ds=ds.map(_parse_data)\n",
        "  ds=ds.shuffle(buffer_size=data_size)\n",
        "  \n",
        "  \n",
        "  ds = ds.repeat()\n",
        "  \n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "  \n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  \n",
        "  return ds\n",
        "  \n",
        "train_ds=_input_fn(x_train,y_train)\n",
        "validation_ds=_input_fn(x_test,y_test)"
      ],
      "metadata": {
        "id": "pKzH02O4YeD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the base model from VGG16 trained convnets"
      ],
      "metadata": {
        "id": "n8RvlY_d4B-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "VGG16_MODEL=tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ],
      "metadata": {
        "id": "tYU6VdnwYw0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_MODEL.trainable=False\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "prediction_layer = tf.keras.layers.Dense(len(label_names),activation='softmax')"
      ],
      "metadata": {
        "id": "ASLRK5b3YNUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  VGG16_MODEL,\n",
        "  global_average_layer,\n",
        "  prediction_layer\n",
        "])"
      ],
      "metadata": {
        "id": "ncwUcE8pY07t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model"
      ],
      "metadata": {
        "id": "cLaUz2ox4Jqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "wGp7LVVVY864"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "cFgleESn4O7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds,\n",
        "                    epochs=100, \n",
        "                    steps_per_epoch=2,\n",
        "                    validation_steps=2,\n",
        "                    validation_data=validation_ds)"
      ],
      "metadata": {
        "id": "zKSShetYY9Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "IBRpcsXa4WrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_steps = 20\n",
        "\n",
        "loss0,accuracy0 = model.evaluate(validation_ds, steps = validation_steps)\n",
        "\n",
        "print(\"loss: {:.2f}\".format(loss0))\n",
        "print(\"accuracy: {:.2f}\".format(accuracy0))"
      ],
      "metadata": {
        "id": "2Fc5eq2C3fLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning curves"
      ],
      "metadata": {
        "id": "Rj7jmx1q4b5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X08burEtZKRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3dlU7LHq3pS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Network in Network(NiN)"
      ],
      "metadata": {
        "id": "DEqMKgYV9vSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.17.5"
      ],
      "metadata": {
        "id": "K4IrxtKt94ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "\n",
        "def nin_block(num_channels, kernel_size, strides, padding):\n",
        "    return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(num_channels, kernel_size, strides=strides,\n",
        "                               padding=padding, activation='relu'),\n",
        "        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\n",
        "                               activation='relu'),\n",
        "        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\n",
        "                               activation='relu')])"
      ],
      "metadata": {
        "id": "4NH1HuKu96PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net():\n",
        "    return tf.keras.models.Sequential([\n",
        "        nin_block(96, kernel_size=11, strides=4, padding='valid'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
        "        nin_block(256, kernel_size=5, strides=1, padding='same'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
        "        nin_block(384, kernel_size=3, strides=1, padding='same'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # There are 10 label classes\n",
        "        nin_block(10, kernel_size=3, strides=1, padding='same'),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Reshape((1, 1, 10)),\n",
        "        # Transform the four-dimensional output into two-dimensional output\n",
        "        # with a shape of (batch size, 10)\n",
        "        tf.keras.layers.Flatten(),\n",
        "        ])"
      ],
      "metadata": {
        "id": "eRqtIUnK96dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.random.uniform((1, 224, 224, 1))\n",
        "for layer in net().layers:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
      ],
      "metadata": {
        "id": "zj8Yurq296kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs, batch_size = 0.1, 10, 128\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "8zDptnl4xYSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google Net"
      ],
      "metadata": {
        "id": "vC4xkIqCD5Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import datasets, layers, models, losses, Model"
      ],
      "metadata": {
        "id": "Hhs45ZlXFgHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()\n",
        "x_train = tf.pad(x_train, [[0, 0], [2,2], [2,2]])/255\n",
        "x_test = tf.pad(x_test, [[0, 0], [2,2], [2,2]])/255\n",
        "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
        "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
        "x_train = tf.repeat(x_train, 3, axis=3)\n",
        "x_test = tf.repeat(x_test, 3, axis=3)\n",
        "x_val = x_train[-2000:,:,:,:]\n",
        "y_val = y_train[-2000:]\n",
        "x_train = x_train[:-2000,:,:,:]\n",
        "y_train = y_train[:-2000]"
      ],
      "metadata": {
        "id": "IE6d6_x1EC4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inception(x,\n",
        "              filters_1x1,\n",
        "              filters_3x3_reduce,\n",
        "              filters_3x3,\n",
        "              filters_5x5_reduce,\n",
        "              filters_5x5,\n",
        "              filters_pool):\n",
        "  path1 = layers.Conv2D(filters_1x1, (1, 1), padding='same',    activation='relu')(x)\n",
        "  path2 = layers.Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu')(x)\n",
        "  path2 = layers.Conv2D(filters_3x3, (1, 1), padding='same', activation='relu')(path2)\n",
        "  path3 = layers.Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu')(x)\n",
        "  path3 = layers.Conv2D(filters_5x5, (1, 1), padding='same', activation='relu')(path3)\n",
        "  path4 = layers.MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "  path4 = layers.Conv2D(filters_pool, (1, 1), padding='same', activation='relu')(path4)\n",
        "  return tf.concat([path1, path2, path3, path4], axis=3)"
      ],
      "metadata": {
        "id": "hmop4Mb1EC8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = layers.Input(shape=(32, 32, 3))\n",
        "input_tensor = layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=x_train.shape[1:])(inp)\n",
        "x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(input_tensor)\n",
        "x = layers.MaxPooling2D(3, strides=2)(x)\n",
        "x = layers.Conv2D(64, 1, strides=1, padding='same', activation='relu')(x)\n",
        "x = layers.Conv2D(192, 3, strides=1, padding='same', activation='relu')(x)\n",
        "x = layers.MaxPooling2D(3, strides=2)(x)\n",
        "x = inception(x, filters_1x1=64, filters_3x3_reduce=96, filters_3x3=128, filters_5x5_reduce=16, filters_5x5=32, filters_pool=32)\n",
        "x = inception(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=192, filters_5x5_reduce=32, filters_5x5=96, filters_pool=64)\n",
        "x = layers.MaxPooling2D(3, strides=2)(x)\n",
        "x = inception(x, filters_1x1=192, filters_3x3_reduce=96, filters_3x3=208, filters_5x5_reduce=16, filters_5x5=48, filters_pool=64)\n",
        "aux1 = layers.AveragePooling2D((5, 5), strides=3)(x)\n",
        "aux1 =layers.Conv2D(128, 1, padding='same', activation='relu')(aux1)\n",
        "aux1 = layers.Flatten()(aux1)\n",
        "aux1 = layers.Dense(1024, activation='relu')(aux1)\n",
        "aux1 = layers.Dropout(0.7)(aux1)\n",
        "aux1 = layers.Dense(10, activation='softmax')(aux1)\n",
        "x = inception(x, filters_1x1=160, filters_3x3_reduce=112, filters_3x3=224, filters_5x5_reduce=24, filters_5x5=64, filters_pool=64)\n",
        "x = inception(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=256, filters_5x5_reduce=24, filters_5x5=64, filters_pool=64)\n",
        "x = inception(x, filters_1x1=112, filters_3x3_reduce=144, filters_3x3=288, filters_5x5_reduce=32, filters_5x5=64, filters_pool=64)\n",
        "aux2 = layers.AveragePooling2D((5, 5), strides=3)(x)\n",
        "aux2 =layers.Conv2D(128, 1, padding='same', activation='relu')(aux2)\n",
        "aux2 = layers.Flatten()(aux2)\n",
        "aux2 = layers.Dense(1024, activation='relu')(aux2)\n",
        "aux2 = layers.Dropout(0.7)(aux2) \n",
        "aux2 = layers.Dense(10, activation='softmax')(aux2)\n",
        "x = inception(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool=128)\n",
        "x = layers.MaxPooling2D(3, strides=2)(x)\n",
        "x = inception(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool=128)\n",
        "x = inception(x, filters_1x1=384, filters_3x3_reduce=192, filters_3x3=384, filters_5x5_reduce=48, filters_5x5=128, filters_pool=128)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "out = layers.Dense(10, activation='softmax')(x)"
      ],
      "metadata": {
        "id": "1eDl0nWvEDBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs = inp, outputs = [out, aux1, aux2])"
      ],
      "metadata": {
        "id": "UhL83HsdEDD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss=[losses.sparse_categorical_crossentropy,\n",
        "                    losses.sparse_categorical_crossentropy,\n",
        "                    losses.sparse_categorical_crossentropy],\n",
        "              loss_weights=[1, 0.3, 0.3],metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gl7Mf8HOEDGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, [y_train, y_train, y_train], validation_data=(x_val, [y_val, y_val, y_val]), batch_size=64, epochs=40)\n"
      ],
      "metadata": {
        "id": "59D88TrWHeKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
        "\n",
        "axs[0].plot(history.history['loss'])\n",
        "axs[0].plot(history.history['val_loss'])\n",
        "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].legend(['Train','Val'])\n",
        "\n",
        "axs[1].plot(history.history['dense_4_accuracy'])\n",
        "axs[1].plot(history.history['val_dense_4_accuracy'])\n",
        "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].legend(['Train', 'Val'])"
      ],
      "metadata": {
        "id": "krNZaUpFMc87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "5arvnwWfMjuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Normalization"
      ],
      "metadata": {
        "id": "LLjd3CDpZZK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "h_cJSW2hMj5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into a DataFrame\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "# Convert datatype to float\n",
        "df = df.astype(float)\n",
        "# append \"target\" and name it \"label\"\n",
        "df['label'] = iris.target\n",
        "# Use string label instead\n",
        "df['label'] = df.label.replace(dict(enumerate(iris.target_names)))"
      ],
      "metadata": {
        "id": "AtucUoCEa09R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "s7FG5UqGc-cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label -> one-hot encoding\n",
        "label = pd.get_dummies(df['label'], prefix='label')\n",
        "df = pd.concat([df, label], axis=1)\n",
        "# drop old label\n",
        "df.drop(['label'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "85hkuyhHa1FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "TldH-6cgdDmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating X and y\n",
        "\n",
        "X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
        "# Convert DataFrame into np array\n",
        "X = np.asarray(X)\n",
        "\n",
        "y = df[['label_setosa', 'label_versicolor', 'label_virginica']]\n",
        "# Convert DataFrame into np array\n",
        "y = np.asarray(y)"
      ],
      "metadata": {
        "id": "BxqaTUS7b5jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data set in a training set (80%) and a test set (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "  X,\n",
        "  y,\n",
        "  test_size=0.20\n",
        ")"
      ],
      "metadata": {
        "id": "zRBoFuygb5lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "_c1yLgIIdYnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizaton"
      ],
      "metadata": {
        "id": "3TIxpexDddlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization"
      ],
      "metadata": {
        "id": "0-JkQzyTdfYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model without batch normalization\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=(4,), activation=\"relu\"),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "]);\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IKyLuOKNd0_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with batch normalization\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=(4,), activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(3, activation='softmax')\n",
        "]);\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PfAjhJ70d1Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import RandomNormal, Constant\n",
        "\n",
        "# Model with a customized batch normalization\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=(4,), activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(\n",
        "        momentum=0.95, \n",
        "        epsilon=0.005,\n",
        "        beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n",
        "        gamma_initializer=Constant(value=0.9)\n",
        "    ),\n",
        "    Dense(3, activation='softmax')\n",
        "]);"
      ],
      "metadata": {
        "id": "k1Ulnbs9d1El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "o4ZOqK2qd1Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "yYNeVHRzeInf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "jjAGxOhub5n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    epochs=200, \n",
        "    validation_split=0.25, \n",
        "    batch_size=40, \n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "HcVTmLahb5rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "def plot_metric(history, metric):\n",
        "    train_metrics = history.history[metric]\n",
        "    val_metrics = history.history['val_'+metric]\n",
        "    epochs = range(1, len(train_metrics) + 1)\n",
        "    plt.plot(epochs, train_metrics)\n",
        "    plt.plot(epochs, val_metrics)\n",
        "    plt.title('Training and validation '+ metric)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TgKDwCeoeTYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(history, 'accuracy')"
      ],
      "metadata": {
        "id": "aBm4fkzTeah6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(history, 'loss')"
      ],
      "metadata": {
        "id": "aKuuN8AWeeGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "model.evaluate(X_test, y_test, verbose=2)"
      ],
      "metadata": {
        "id": "Qor3yvpkefGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ResNet"
      ],
      "metadata": {
        "id": "LnqllG2yZq0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.17.5"
      ],
      "metadata": {
        "id": "dvgAbPf0u2XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "\n",
        "class Residual(tf.keras.Model):  #save\n",
        "    \"\"\"The Residual block of ResNet.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "            num_channels, padding='same', kernel_size=3, strides=strides)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "            num_channels, kernel_size=3, padding='same')\n",
        "        self.conv3 = None\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = tf.keras.layers.Conv2D(\n",
        "                num_channels, kernel_size=1, strides=strides)\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    def call(self, X):\n",
        "        Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3 is not None:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return tf.keras.activations.relu(Y)"
      ],
      "metadata": {
        "id": "wug4oHXnu_86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(3)\n",
        "X = tf.random.uniform((4, 6, 6, 3))\n",
        "Y = blk(X)\n",
        "Y.shape"
      ],
      "metadata": {
        "id": "yNkWEXutvAeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "M-SF721LvKHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b1 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])"
      ],
      "metadata": {
        "id": "4fmd5TnivRhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_channels, num_residuals, first_block=False,\n",
        "                 **kwargs):\n",
        "        super(ResnetBlock, self).__init__(**kwargs)\n",
        "        self.residual_layers = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                self.residual_layers.append(\n",
        "                    Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                self.residual_layers.append(Residual(num_channels))\n",
        "\n",
        "    def call(self, X):\n",
        "        for layer in self.residual_layers.layers:\n",
        "            X = layer(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "JqWFp5xIvUMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b2 = ResnetBlock(64, 2, first_block=True)\n",
        "b3 = ResnetBlock(128, 2)\n",
        "b4 = ResnetBlock(256, 2)\n",
        "b5 = ResnetBlock(512, 2)"
      ],
      "metadata": {
        "id": "LFLp7MgKvYIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall that we define this as a function so we can reuse later and run it\n",
        "# within `tf.distribute.MirroredStrategy`'s scope to utilize various\n",
        "# computational resources, e.g. GPUs. Also note that even though we have\n",
        "# created b1, b2, b3, b4, b5 but we will recreate them inside this function's\n",
        "# scope instead\n",
        "def net():\n",
        "    return tf.keras.Sequential([\n",
        "        # The following layers are the same as b1 that we created earlier\n",
        "        tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
        "        # The following layers are the same as b2, b3, b4, and b5 that we\n",
        "        # created earlier\n",
        "        ResnetBlock(64, 2, first_block=True),\n",
        "        ResnetBlock(128, 2),\n",
        "        ResnetBlock(256, 2),\n",
        "        ResnetBlock(512, 2),\n",
        "        tf.keras.layers.GlobalAvgPool2D(),\n",
        "        tf.keras.layers.Dense(units=10)])"
      ],
      "metadata": {
        "id": "yl1LIr5Pvau7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.random.uniform(shape=(1, 224, 224, 1))\n",
        "for layer in net().layers:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
      ],
      "metadata": {
        "id": "zuqGTluZvduw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs, batch_size = 0.05, 10, 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "OZhWn7Rowls5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DenseNet"
      ],
      "metadata": {
        "id": "jALEKtFFZxSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.17.5"
      ],
      "metadata": {
        "id": "eREdsgrcZ39T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "\n",
        "class ConvBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.bn = tf.keras.layers.BatchNormalization()\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            filters=num_channels, kernel_size=(3, 3), padding='same')\n",
        "\n",
        "        self.listLayers = [self.bn, self.relu, self.conv]\n",
        "\n",
        "    def call(self, x):\n",
        "        y = x\n",
        "        for layer in self.listLayers.layers:\n",
        "            y = layer(y)\n",
        "        y = tf.keras.layers.concatenate([x,y], axis=-1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "1a-AyoyRwVsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_convs, num_channels):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.listLayers = []\n",
        "        for _ in range(num_convs):\n",
        "            self.listLayers.append(ConvBlock(num_channels))\n",
        "\n",
        "    def call(self, x):\n",
        "        for layer in self.listLayers.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rbHJ8t3_riUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = DenseBlock(2, 10)\n",
        "X = tf.random.uniform((4, 8, 8, 3))\n",
        "Y = blk(X)\n",
        "Y.shape"
      ],
      "metadata": {
        "id": "J1a1rT1IrleY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransitionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_channels, **kwargs):\n",
        "        super(TransitionBlock, self).__init__(**kwargs)\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        self.conv = tf.keras.layers.Conv2D(num_channels, kernel_size=1)\n",
        "        self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv(x)\n",
        "        return self.avg_pool(x)"
      ],
      "metadata": {
        "id": "Y90JYQPPrpGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = TransitionBlock(10)\n",
        "blk(Y).shape"
      ],
      "metadata": {
        "id": "udGJ4i7MxGEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def block_1():\n",
        "    return tf.keras.Sequential([\n",
        "       tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same'),\n",
        "       tf.keras.layers.BatchNormalization(),\n",
        "       tf.keras.layers.ReLU(),\n",
        "       tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])"
      ],
      "metadata": {
        "id": "VaXePLuHxIVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def block_2():\n",
        "    net = block_1()\n",
        "    # `num_channels`: the current number of channels\n",
        "    num_channels, growth_rate = 64, 32\n",
        "    num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
        "\n",
        "    for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
        "        net.add(DenseBlock(num_convs, growth_rate))\n",
        "        # This is the number of output channels in the previous dense block\n",
        "        num_channels += num_convs * growth_rate\n",
        "        # A transition layer that halves the number of channels is added\n",
        "        # between the dense blocks\n",
        "        if i != len(num_convs_in_dense_blocks) - 1:\n",
        "            num_channels //= 2\n",
        "            net.add(TransitionBlock(num_channels))\n",
        "    return net"
      ],
      "metadata": {
        "id": "3NGhqnn3xKoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net():\n",
        "    net = block_2()\n",
        "    net.add(tf.keras.layers.BatchNormalization())\n",
        "    net.add(tf.keras.layers.ReLU())\n",
        "    net.add(tf.keras.layers.GlobalAvgPool2D())\n",
        "    net.add(tf.keras.layers.Flatten())\n",
        "    net.add(tf.keras.layers.Dense(10))\n",
        "    return net"
      ],
      "metadata": {
        "id": "b5vFAaHKxMv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs, batch_size = 0.1, 10, 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "xTSPW_fux2o6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}